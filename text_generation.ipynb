{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hchgssarwyh/ai_fiction/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrfXFSS0xrwn"
      },
      "source": [
        "# Char-based text generation with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKfLxpcwxrws"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC8PLPr6xrwu",
        "outputId": "a669700d-a9fb-4804-a135-bb93cdd6d7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '\\n', 'о', 'а', 'е', 'т', 'и', 'н', 'л', 'с', 'р', 'в', 'к', 'у', 'д', 'м', 'п', 'ь', 'я', 'ы', '.', 'й', 'г', 'б', 'з', ',', 'ч', 'ж', 'х', 'ш', 'ю', '!', 'П', 'С', 'В', '…', 'Н', '—', 'ц', 'К', '?', 'О', 'Т', 'М', 'Д', 'Г', 'И', 'З', 'Б', 'Л', 'А', 'щ', '-', ':', '»', '«', 'У', 'Р', 'Ч', 'Х', 'э', 'Я', 'Ж', 'ф', 'Е', '*', 'Ш', 'Ф', 'Э', 'ё', '–', 'Ц', 'e', '0', 'o', 'ъ', 't', 'a', '(', 'n', 'k', ')', 'C', 'X', 'r', '\"', 'Й', 'c', 'Щ', \"'\", 'p', 'm', 'Ь', 's', 'i', '9', '1', 'w', 'B', 'M', 'd', '2', 'Ю', 'N', 'b', 'u', 'D', 'l', 'K', 'Ё', 'V', 'R', 'h', 'U', 'I', '<', '8', '4', '#', 'H', '6', '7']\n"
          ]
        }
      ],
      "source": [
        "TRAIN_TEXT_FILE_PATH = 'train_text.txt'\n",
        "\n",
        "with open(TRAIN_TEXT_FILE_PATH) as text_file:\n",
        "    text_sample = text_file.readlines()\n",
        "text_sample = ' '.join(text_sample)\n",
        "\n",
        "def text_to_seq(text_sample):\n",
        "    char_counts = Counter(text_sample)\n",
        "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    sorted_chars = [char for char, _ in char_counts]\n",
        "    print(sorted_chars)\n",
        "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
        "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
        "    \n",
        "    return sequence, char_to_idx, idx_to_char\n",
        "\n",
        "sequence, char_to_idx, idx_to_char = text_to_seq(text_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzXY8Ux0xrwu"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_batch(sequence):\n",
        "    trains = []\n",
        "    targets = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN)\n",
        "        chunk = sequence[batch_start: batch_start + SEQ_LEN]\n",
        "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
        "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
        "        trains.append(train)\n",
        "        targets.append(target)\n",
        "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX_n1uBDxrwv"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "    \n",
        "    _, hidden = model(train, hidden)\n",
        "        \n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "    \n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "    \n",
        "    return predicted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_SNA1itxrww"
      },
      "outputs": [],
      "source": [
        "class TextRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x).squeeze(2)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekz3QQtGxrwx",
        "outputId": "857f6e08-8548-4a19-e0af-54cf44eb2892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.944972162246704\n",
            "Loss: 2.4089386653900147\n",
            "Loss: 2.2373326921463015\n",
            "Loss: 2.1201955461502076\n",
            "Loss: 2.03681024312973\n",
            "Loss: 1.95648743391037\n",
            "Loss: 1.8821537971496582\n",
            "Loss: 1.8260656023025512\n",
            "Loss: 1.7737565517425538\n",
            "Loss: 1.7200907063484192\n",
            "Loss: 1.6762205815315248\n",
            "Loss: 1.6482713985443116\n",
            "Loss: 1.599206771850586\n",
            "Loss: 1.5734911823272706\n",
            "Loss: 1.5496368741989135\n",
            "Loss: 1.5120799112319947\n",
            "Loss: 1.4862719368934632\n",
            "Loss: 1.4616685485839844\n",
            "Loss: 1.4395147156715393\n",
            "Loss: 1.4229584002494813\n",
            "Loss: 1.402214617729187\n",
            "Loss: 1.3805783343315126\n",
            "Loss: 1.3670485258102416\n",
            "Loss: 1.3429681253433228\n",
            "Loss: 1.322350583076477\n",
            "Loss: 1.3170025777816772\n",
            "Loss: 1.2920494699478149\n",
            "Loss: 1.293750114440918\n",
            "Loss: 1.278380992412567\n",
            "Loss: 1.2762347912788392\n",
            "Loss: 1.2575621700286865\n",
            "Loss: 1.2510270404815673\n",
            "Loss: 1.2474600863456726\n",
            "Loss: 1.2373412466049194\n",
            "Loss: 1.221976134777069\n",
            "Loss: 1.1957749271392821\n",
            "Loss: 1.2147232460975648\n",
            "Loss: 1.1863037896156312\n",
            "Loss: 1.1874105834960937\n",
            "Loss: 1.1902997851371766\n",
            "Loss: 1.1684455752372742\n",
            "Loss: 1.164844799041748\n",
            "Loss: 1.1525985884666443\n",
            "Loss: 1.1669194555282594\n",
            "Loss: 1.1575540041923522\n",
            "Loss: 1.1427211356163025\n",
            "Loss: 1.1383137917518615\n",
            "Loss: 1.1200275182724\n",
            "Loss: 1.1308181262016297\n",
            "Loss: 1.1266270685195923\n",
            "Loss: 1.1184383749961853\n",
            "Loss: 1.1084685730934143\n",
            "Loss: 1.11268141746521\n",
            "Loss: 1.094968945980072\n",
            "Loss: 1.108693735599518\n",
            "Loss: 1.0978288388252258\n",
            "Loss: 1.0912449073791504\n",
            "Loss: 1.083740406036377\n",
            "Loss: 1.0792575192451477\n",
            "Loss: 1.0854193115234374\n",
            "Loss: 1.0708723449707032\n",
            "Loss: 1.066407597064972\n",
            "Loss: 1.0672625184059144\n",
            "Loss: 1.0578811657428742\n",
            "Loss: 1.0790383529663086\n",
            "Loss: 1.064362485408783\n",
            "Loss: 1.051837693452835\n",
            "Loss: 1.0681593275070191\n",
            "Loss: 1.057684895992279\n",
            "Loss: 1.0521534752845765\n",
            "Loss: 1.0528913486003875\n",
            "Loss: 1.0414237236976625\n",
            "Loss: 1.0452417385578157\n",
            "Loss: 1.0331265425682068\n",
            "Loss: 1.0371722650527955\n",
            "Loss: 1.0424078321456909\n",
            "Loss: 1.0360259521007538\n",
            "Loss: 1.0284629189968109\n",
            "Loss: 1.0203535604476928\n",
            "Loss: 1.0304302191734314\n",
            "Loss: 1.0218346309661865\n",
            "Loss: 1.006082056760788\n",
            "Loss: 1.0122900569438935\n",
            "Loss: 1.0137009692192078\n",
            "Loss: 1.0122063529491425\n",
            "Loss: 1.0039762341976166\n",
            "Loss: 1.0099141192436218\n",
            "Loss: 1.0044040822982787\n",
            "Loss: 1.0109485244750978\n",
            "Loss: 1.008330111503601\n",
            "Loss: 1.0097636651992798\n",
            "Loss: 1.0067885649204253\n",
            "Epoch 00092: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Loss: 0.9186952793598175\n",
            "Loss: 0.8512833023071289\n",
            "Loss: 0.8313945960998536\n",
            "Loss: 0.8157874810695648\n",
            "Loss: 0.813811936378479\n",
            "Loss: 0.7993198692798614\n",
            "Loss: 0.7981299436092377\n",
            "Loss: 0.7928069722652435\n",
            "Loss: 0.7749089765548706\n",
            "Loss: 0.7795920729637146\n",
            "Loss: 0.7763220918178558\n",
            "Loss: 0.7849764192104339\n",
            "Loss: 0.7761066055297852\n",
            "Loss: 0.7749503934383393\n",
            "Loss: 0.7829988706111908\n",
            "Epoch 00107: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Loss: 0.7350398766994476\n",
            "Loss: 0.6999396550655365\n",
            "Loss: 0.6946320140361786\n",
            "Loss: 0.6911594974994659\n",
            "Loss: 0.6792745518684388\n",
            "Loss: 0.68040083527565\n",
            "Loss: 0.68047159075737\n",
            "Loss: 0.6738977980613708\n",
            "Loss: 0.6781861579418182\n",
            "Loss: 0.668923397064209\n",
            "Loss: 0.6652486145496368\n",
            "Loss: 0.6669714069366455\n",
            "Loss: 0.6640112578868866\n",
            "Loss: 0.6650135242938995\n",
            "Loss: 0.6568731153011322\n",
            "Loss: 0.6575010120868683\n",
            "Loss: 0.6526786863803864\n",
            "Loss: 0.6512487018108368\n",
            "Loss: 0.6516231000423431\n",
            "Loss: 0.6520617401599884\n",
            "Loss: 0.6491061961650848\n",
            "Loss: 0.6448657071590423\n",
            "Loss: 0.6432428777217865\n",
            "Loss: 0.6408234512805939\n",
            "Loss: 0.6416622090339661\n",
            "Loss: 0.6458273446559906\n",
            "Loss: 0.6458755588531494\n",
            "Loss: 0.6432528209686279\n",
            "Loss: 0.6423279404640198\n",
            "Loss: 0.6430212676525116\n",
            "Epoch 00137: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Loss: 0.6147318172454834\n",
            "Loss: 0.6007875680923462\n",
            "Loss: 0.5993005621433258\n",
            "Loss: 0.5909459102153778\n",
            "Loss: 0.5875784623622894\n",
            "Loss: 0.5883801078796387\n",
            "Loss: 0.5848718011379241\n",
            "Loss: 0.5828730547428131\n",
            "Loss: 0.5824661421775817\n",
            "Loss: 0.5823336899280548\n",
            "Loss: 0.5776122844219208\n",
            "Loss: 0.5786399865150451\n",
            "Loss: 0.5829683494567871\n",
            "Loss: 0.5822660005092621\n",
            "Loss: 0.5771320378780365\n",
            "Loss: 0.579568098783493\n",
            "Loss: 0.5726021957397461\n",
            "Loss: 0.5756336569786071\n",
            "Loss: 0.575200811624527\n",
            "Loss: 0.5704715156555176\n",
            "Loss: 0.5763047695159912\n",
            "Loss: 0.5679125237464905\n",
            "Loss: 0.5693660759925843\n",
            "Loss: 0.5711078011989593\n",
            "Loss: 0.5666581332683563\n",
            "Loss: 0.5657625555992126\n",
            "Loss: 0.5610638642311097\n",
            "Loss: 0.569899936914444\n",
            "Loss: 0.5689467692375183\n",
            "Loss: 0.567204213142395\n",
            "Loss: 0.5598984217643738\n",
            "Loss: 0.5609055745601654\n",
            "Loss: 0.5579575598239899\n",
            "Loss: 0.5614529907703399\n",
            "Loss: 0.5578879046440125\n",
            "Loss: 0.564566296339035\n",
            "Loss: 0.558154102563858\n",
            "Loss: 0.5594573879241943\n",
            "Loss: 0.5570974159240722\n",
            "Loss: 0.5605115222930909\n",
            "Loss: 0.5621080362796783\n",
            "Loss: 0.5551253545284272\n",
            "Loss: 0.554281256198883\n",
            "Loss: 0.5506220626831054\n",
            "Loss: 0.5542380094528199\n",
            "Loss: 0.5533297729492187\n",
            "Loss: 0.5518514108657837\n",
            "Loss: 0.549816927909851\n",
            "Loss: 0.5528256916999816\n",
            "Loss: 0.5497159945964813\n",
            "Loss: 0.545403026342392\n",
            "Loss: 0.5430315971374512\n",
            "Loss: 0.5464461016654968\n",
            "Loss: 0.5482452058792114\n",
            "Loss: 0.5520909070968628\n",
            "Loss: 0.5448442029953003\n",
            "Loss: 0.5451344513893127\n",
            "Loss: 0.5453485453128815\n",
            "Epoch 00195: reducing learning rate of group 0 to 6.2500e-04.\n",
            "Loss: 0.5286498379707336\n",
            "Loss: 0.5263892120122909\n",
            "Loss: 0.5237536990642547\n",
            "Loss: 0.5245132523775101\n",
            "Loss: 0.5180292534828186\n",
            "Loss: 0.5182056307792664\n",
            "Loss: 0.5180167961120605\n",
            "Loss: 0.5213097685575485\n",
            "Loss: 0.5155478805303574\n",
            "Loss: 0.5151193976402283\n",
            "Loss: 0.5176936256885528\n",
            "Loss: 0.5158885204792023\n",
            "Loss: 0.5166738224029541\n",
            "Loss: 0.5140300750732422\n",
            "Loss: 0.5106172508001328\n",
            "Loss: 0.514536565542221\n",
            "Loss: 0.5107320672273636\n",
            "Loss: 0.5137529730796814\n",
            "Loss: 0.5077104187011718\n",
            "Loss: 0.5161860901117324\n",
            "Loss: 0.5108211725950241\n",
            "Loss: 0.5091931712627411\n",
            "Loss: 0.5108795201778412\n",
            "Loss: 0.5102727711200714\n",
            "Loss: 0.5129390525817871\n",
            "Epoch 00220: reducing learning rate of group 0 to 3.1250e-04.\n",
            "Loss: 0.5092980045080185\n",
            "Loss: 0.5084223031997681\n",
            "Loss: 0.5011941361427307\n",
            "Loss: 0.49615686118602753\n",
            "Loss: 0.5074021291732788\n",
            "Loss: 0.4975156545639038\n",
            "Loss: 0.49924057245254516\n",
            "Loss: 0.5020112860202789\n",
            "Loss: 0.4999604022502899\n",
            "Loss: 0.49820197224617\n",
            "Epoch 00230: reducing learning rate of group 0 to 1.5625e-04.\n",
            "Loss: 0.4961740118265152\n",
            "Loss: 0.4910697340965271\n",
            "Loss: 0.4959181576967239\n",
            "Loss: 0.4925831627845764\n",
            "Loss: 0.4947402596473694\n",
            "Loss: 0.49442593097686766\n",
            "Loss: 0.4989120489358902\n",
            "Loss: 0.49413935601711273\n",
            "Epoch 00238: reducing learning rate of group 0 to 7.8125e-05.\n",
            "Loss: 0.4934306180477142\n",
            "Loss: 0.4903402906656265\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    patience=5, \n",
        "    verbose=True, \n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 12000\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "       # print(predicted_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot;\n",
        "from telebot import types\n",
        "bot = telebot.TeleBot('5912676709:AAFogJDypnO9nA9GZ9bKMB4Kw4ul_d7CYQk');\n",
        "\n",
        "word = '';\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def get_first_messages(message):\n",
        "    if message.text == \"Напиши стих\":\n",
        "        bot.send_message(message.from_user.id, \"Введи первое слово\")\n",
        "        bot.register_next_step_handler(message, get_word);\n",
        "    elif message.text == \"/help\":\n",
        "        bot.send_message(message.from_user.id, 'Спроси \"Напиши стих\"')\n",
        "    else:\n",
        "        bot.send_message(message.from_user.id, \"Я тебя не понимаю. Напиши /help.\")\n",
        "\n",
        "def get_word(message): #получаем слово\n",
        "    global word;\n",
        "    word = message.text;\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    bot.send_message(message.from_user.id, \n",
        "    evaluate(\n",
        "    model, \n",
        "    char_to_idx, \n",
        "    idx_to_char, \n",
        "    temp=0.3, \n",
        "    prediction_len=200, \n",
        "    start_text = word + ' '\n",
        "    ))\n",
        "\n",
        "    keyboard = types.InlineKeyboardMarkup(); #наша клавиатура\n",
        "    key_yes = types.InlineKeyboardButton(text='Да', callback_data='yes'); #кнопка «Да»\n",
        "    keyboard.add(key_yes); #добавляем кнопку в клавиатуру\n",
        "    key_no= types.InlineKeyboardButton(text='Нет', callback_data='no');\n",
        "    keyboard.add(key_no);\n",
        "    question = 'Попробовать еще?';\n",
        "    bot.send_message(message.from_user.id, text=question, reply_markup=keyboard)\n",
        "\n",
        "@bot.callback_query_handler(func=lambda call: True)\n",
        "def callback_worker(call):\n",
        "    if call.data == \"yes\": #call.data это callback_data, которую мы указали при объявлении кнопки\n",
        "        bot.send_message(call.message.chat.id, 'Введи первое слово');\n",
        "     #   message = message.text\n",
        "        bot.register_next_step_handler(call.message, get_word);\n",
        "    elif call.data == \"no\":\n",
        "        bot.send_message(call.message.chat.id, ':(');"
      ],
      "metadata": {
        "id": "A249HoD3BFTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.polling(none_stop=True, interval=0) "
      ],
      "metadata": {
        "id": "X2SvyYifEuwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}