{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hchgssarwyh/ai_fiction/blob/features/tg_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrfXFSS0xrwn"
      },
      "source": [
        "# Telegram bot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "c8GbzKxDIEXN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x).squeeze(2)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))"
      ],
      "metadata": {
        "id": "I5EIr1JXIEAL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate1(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    with open('idx_to_char_hokku.pickle', 'rb') as f:\n",
        "      idx_to_char = pickle.load(f)\n",
        "    with open('char_to_idx_hokku.pickle', 'rb') as f:\n",
        "      char_to_idx = pickle.load(f)\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "    \n",
        "    _, hidden = model(train, hidden)\n",
        "        \n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "    \n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "    \n",
        "    return predicted_text"
      ],
      "metadata": {
        "id": "ubPqoTa0Igpg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate2(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    with open('idx_to_char_harms.pickle', 'rb') as f:\n",
        "      idx_to_char = pickle.load(f)\n",
        "    with open('char_to_idx_harms.pickle', 'rb') as f:\n",
        "      char_to_idx = pickle.load(f)\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "    \n",
        "    _, hidden = model(train, hidden)\n",
        "        \n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "    \n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "    \n",
        "    return predicted_text"
      ],
      "metadata": {
        "id": "HDWQiIv3Kizo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate3(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    with open('idx_to_char_sesenin.pickle', 'rb') as f:\n",
        "      idx_to_char = pickle.load(f)\n",
        "    with open('char_to_idx_sesenin.pickle', 'rb') as f:\n",
        "      char_to_idx = pickle.load(f)\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "    \n",
        "    _, hidden = model(train, hidden)\n",
        "        \n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "    \n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "    \n",
        "    return predicted_text"
      ],
      "metadata": {
        "id": "55utUkIJKmKL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('idx_to_char_hokku.pickle', 'rb') as f:\n",
        "  idx_to_char_hokku = pickle.load(f)\n",
        "model_hokku = TextRNN(input_size=len(idx_to_char_hokku), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model_hokku.load_state_dict(torch.load('model_hokku'))\n",
        "model_hokku.eval()"
      ],
      "metadata": {
        "id": "TAkMre1zIh9j",
        "outputId": "3d0184ba-8a94-43bc-b0ac-6f8aa4a9d4d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextRNN(\n",
              "  (encoder): Embedding(70, 128)\n",
              "  (lstm): LSTM(128, 128, num_layers=2)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=70, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('idx_to_char_harms.pickle', 'rb') as f:\n",
        "  idx_to_char_harms = pickle.load(f)\n",
        "model_harms = TextRNN(input_size=len(idx_to_char_harms), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model_harms.load_state_dict(torch.load('modeld_harms'))\n",
        "model_harms.eval()"
      ],
      "metadata": {
        "id": "bkB-SwEeIl-g",
        "outputId": "9680956e-0071-4665-ac94-b0e4706c57f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextRNN(\n",
              "  (encoder): Embedding(84, 128)\n",
              "  (lstm): LSTM(128, 128, num_layers=2)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('idx_to_char_sesenin.pickle', 'rb') as f:\n",
        "  idx_to_char_sesenin = pickle.load(f)\n",
        "model_sesenin = TextRNN(input_size=len(idx_to_char_sesenin), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model_sesenin.load_state_dict(torch.load('modeld_sesenin'))\n",
        "model_sesenin.eval()"
      ],
      "metadata": {
        "id": "hFdAD_DbImv-",
        "outputId": "516aaa3c-d2a9-4c44-aa70-891a63b5e103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-2d199e7fdece>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0midx_to_char_sesenin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_sesenin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_char_sesenin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_sesenin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modeld_sesenin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_sesenin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TextRNN:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([800, 128]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([800, 200]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([800]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([800]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([800, 200]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([800, 200]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([800]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([800]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([105, 200]) from checkpoint, the shape in current model is torch.Size([105, 128])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_hokku.to(device)\n",
        "model_harms.to(device)\n",
        "model_sesenin.to(device)"
      ],
      "metadata": {
        "id": "JZRO3wRtJBtX",
        "outputId": "f402e026-ff62-4fd3-cb2b-eed8ead37ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextRNN(\n",
              "  (encoder): Embedding(105, 128)\n",
              "  (lstm): LSTM(128, 128, num_layers=2)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=105, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def poem_hokku(start):\n",
        "  with open('idx_to_char_hokku.pickle', 'rb') as f:\n",
        "    idx_to_char_hokku = pickle.load(f)\n",
        "  with open('char_to_idx_hokku.pickle', 'rb') as f:\n",
        "    char_to_idx_hokku = pickle.load(f)\n",
        "  print(evaluate1(\n",
        "      model_hokku, \n",
        "      char_to_idx_hokku, \n",
        "      idx_to_char_hokku, \n",
        "      temp=0.3, \n",
        "      prediction_len=200, \n",
        "      start_text=str(start)+' '\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "nC-3s5ZiJKGk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poem_harms(start):\n",
        "  with open('idx_to_char_harms.pickle', 'rb') as f:\n",
        "    idx_to_char_harms = pickle.load(f)\n",
        "  with open('char_to_idx_harms.pickle', 'rb') as f:\n",
        "    char_to_idx_harms = pickle.load(f)\n",
        "  print(evaluate2(\n",
        "      model_harms, \n",
        "      char_to_idx_harms, \n",
        "      idx_to_char_harms, \n",
        "      temp=0.3, \n",
        "      prediction_len=200, \n",
        "      start_text=str(start)+' '\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "L-Czw-QlJq0F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poem_sesenin(start):\n",
        "  with open('idx_to_char_sesenin.pickle', 'rb') as f:\n",
        "    idx_to_char_sesenin = pickle.load(f)\n",
        "  with open('char_to_idx_sesenin.pickle', 'rb') as f:\n",
        "    char_to_idx_sesenin = pickle.load(f)\n",
        "  print(evaluate3(\n",
        "      model_sesenin, \n",
        "      char_to_idx_sesenin, \n",
        "      idx_to_char_sesenin, \n",
        "      temp=0.3, \n",
        "      prediction_len=200, \n",
        "      start_text=str(start)+' '\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "sX6LoanhJ4lG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poem_harms('всем привет')"
      ],
      "metadata": {
        "id": "yfj1ZClOKEnw",
        "outputId": "d50a0c13-7f31-4fa9-dddd-38b59ddaab88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всем привет   и небо…\n",
            " Сынул на траву.\n",
            " Вот и солнце сказал и душейно.\n",
            " Вдруг и двались обрада\n",
            " снился папа мой\n",
            " Потому что\n",
            " День\n",
            "     народ листик дубовый\n",
            " Лезет на песке.\n",
            " Мы говорили: это круг,\n",
            " а теперь мне в\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hchgssarwyh/ai_fiction/blob/features/telebot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrfXFSS0xrwn"
      },
      "source": [
        "# Telegram bot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot;\n",
        "from telebot import types\n",
        "\n"
      ],
      "metadata": {
        "id": "Kks1e_a9BhBP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = telebot.TeleBot('5912676709:AAFogJDypnO9nA9GZ9bKMB4Kw4ul_d7CYQk');\n",
        "word = '';\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def get_first_messages(message):\n",
        "\n",
        "\n",
        "    if message.text == \"/help\":\n",
        "        bot.send_message(message.from_user.id, 'Список команд: \\n /author - выбор стиля для стиха или прозы \\n /poem - выбор первого слова для начала стиха')\n",
        "    elif message.text == \"/poem\":\n",
        "        bot.send_message(message.from_user.id, \"Введи первое слово\")\n",
        "        bot.register_next_step_handler(message, get_word);\n",
        "    \n",
        "    elif message.text == \"/author\":\n",
        "\n",
        "        keyboard_author = types.InlineKeyboardMarkup(); #наша клавиатура\n",
        "\n",
        "        key_hokku = types.InlineKeyboardButton(text='Хокку', callback_data='hokku');\n",
        "        keyboard_author.add(key_hokku); #добавляем кнопку c хокку\n",
        "        key_esenin= types.InlineKeyboardButton(text='Есенин', callback_data='esenin');\n",
        "        keyboard_author.add(key_esenin);#добавляем кнопку c Есенином\n",
        "        key_harms= types.InlineKeyboardButton(text='Хармс', callback_data='harms');\n",
        "        keyboard_author.add(key_harms);#добавляем кнопку c Хармсом\n",
        "\n",
        "        header = 'Список стилей';\n",
        "        bot.send_message(message.from_user.id, text=header, reply_markup=keyboard_author)\n",
        "\n",
        "    elif message.text == \"/start\":\n",
        "        bot.send_message(message.from_user.id,'Отправь /poem и я сочиню для тебя что то!' )\n",
        "    else:\n",
        "        bot.send_message(message.from_user.id, \"Я тебя не понимаю. Напиши /help.\")\n",
        "\n",
        "\n",
        "def get_word(message): #получаем слово\n",
        "    global word;\n",
        "    word = message.text;\n",
        "\n",
        "    bot.send_message(message.from_user.id, word + \" я люблю \\n И кушать кашу я люблю \\n Вот такой вот белый стих\")\n",
        "\n",
        "    keyboard_word = types.InlineKeyboardMarkup(); #наша клавиатура\n",
        "\n",
        "    key_yes = types.InlineKeyboardButton(text='Да', callback_data='yes'); #кнопка «Да»\n",
        "    keyboard_word.add(key_yes); #добавляем кнопку в клавиатуру\n",
        "    key_no= types.InlineKeyboardButton(text='Нет', callback_data='no');\n",
        "    keyboard_word.add(key_no);\n",
        "    question = 'Попробовать еще?';\n",
        "    bot.send_message(message.from_user.id, text=question, reply_markup=keyboard_word)\n",
        "\n",
        "\n",
        "@bot.callback_query_handler(func=lambda call: call.data == \"yes\" or call.data == 'no')\n",
        "\n",
        "def callback_worker(call):\n",
        "    if call.data == \"yes\": #call.data это callback_data, которую мы указали при объявлении кнопки\n",
        "        bot.send_message(call.message.chat.id, 'Введи первое слово');\n",
        "        bot.register_next_step_handler(call.message, get_word);\n",
        "    elif call.data == \"no\":\n",
        "        bot.send_message(call.message.chat.id, ':(');\n",
        "\n",
        "@bot.callback_query_handler(func=lambda call: call.data == \"hokku\" or call.data == 'esenin' or call.data == 'harms')\n",
        "def callback_author(call):\n",
        "    if call.data == \"hokku\": \n",
        "        bot.send_message(call.message.chat.id, 'ok');\n",
        "        bot.send_message(call.message.chat.id, 'Введи первое слово');\n",
        "        bot.register_next_step_handler(call.message, get_word);\n",
        "    elif call.data == \"esenin\":\n",
        "        bot.send_message(call.message.chat.id, 'ok2');\n",
        "        bot.send_message(call.message.chat.id, 'Введи первое слово');\n",
        "        bot.register_next_step_handler(call.message, get_word);\n",
        "    elif call.data == \"harms\":\n",
        "        bot.send_message(call.message.chat.id, 'ok3');\n",
        "        bot.send_message(call.message.chat.id, 'Введи первое слово');\n",
        "        bot.register_next_step_handler(call.message, get_word);\n",
        "\n"
      ],
      "metadata": {
        "id": "A249HoD3BFTw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot.polling(none_stop=True, interval=0) "
      ],
      "metadata": {
        "id": "X2SvyYifEuwf"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
